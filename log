[
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503471780",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503471780",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503471780,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzQ3MTc4MA==",
    "user": {
      "login": "drezap",
      "id": 11656812,
      "node_id": "MDQ6VXNlcjExNjU2ODEy",
      "avatar_url": "https://avatars2.githubusercontent.com/u/11656812?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/drezap",
      "html_url": "https://github.com/drezap",
      "followers_url": "https://api.github.com/users/drezap/followers",
      "following_url": "https://api.github.com/users/drezap/following{/other_user}",
      "gists_url": "https://api.github.com/users/drezap/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/drezap/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/drezap/subscriptions",
      "organizations_url": "https://api.github.com/users/drezap/orgs",
      "repos_url": "https://api.github.com/users/drezap/repos",
      "events_url": "https://api.github.com/users/drezap/events{/privacy}",
      "received_events_url": "https://api.github.com/users/drezap/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T08:57:18Z",
    "updated_at": "2019-06-19T08:57:18Z",
    "author_association": "COLLABORATOR",
    "body": "So I just want to understand why this is important, I want to develop some literacy. \r\n\r\nSo openCL is the kernel that manages operations on the GPU. This pull request is to enable operations having to do with PSD or triangular matrices, on the GPU. Exploiting triangular matrices has large speed gains and memory gains. For a simple example, we've added GPU operations for addition for triangular matrices. It's obviously faster to sum the n(n-1)/2 operations than sum half of (or half +/1 the diagonal) a matrix of zeros. But these speed gains are even better on the GPU, could be useful for any matrix operations in the future. Just very generally applicable. \r\n\r\nIs this correct? I can read the code to see how it's functioning, but hard to see how to improve it besides style, so I'm not really sure I can review this."
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503515432",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503515432",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503515432,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzUxNTQzMg==",
    "user": {
      "login": "t4c1",
      "id": 7499452,
      "node_id": "MDQ6VXNlcjc0OTk0NTI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/7499452?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/t4c1",
      "html_url": "https://github.com/t4c1",
      "followers_url": "https://api.github.com/users/t4c1/followers",
      "following_url": "https://api.github.com/users/t4c1/following{/other_user}",
      "gists_url": "https://api.github.com/users/t4c1/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/t4c1/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/t4c1/subscriptions",
      "organizations_url": "https://api.github.com/users/t4c1/orgs",
      "repos_url": "https://api.github.com/users/t4c1/repos",
      "events_url": "https://api.github.com/users/t4c1/events{/privacy}",
      "received_events_url": "https://api.github.com/users/t4c1/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T11:09:11Z",
    "updated_at": "2019-06-19T11:10:08Z",
    "author_association": "CONTRIBUTOR",
    "body": "> Is this correct?\r\n\r\nNo. First there are no memory gains. We still store whole matrix, even if we need only a triangular matrix. This is simpler and probably does not affect performance that much. Operations that are faster on triangular matrices were optimized in this way before this PR. \r\n\r\nWhat this PR changes is that it lets the matrix track which part of it is actually in use. So that does not need to be specified when calling a function that operates on a triangular part. For example addition of two upper triangular matrices produces an upper triangular matrix. Some functions needed to be changed to only operate on triangular part if it is specified on input matrix. However I expect no or minimal benefits to performance.\r\n\r\nIn OpenCL terminology a kernel is any function that is called from host (CPU) and executes on OpenCL device (GPU). I don't know what PSD is."
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503661160",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503661160",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503661160,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzY2MTE2MA==",
    "user": {
      "login": "bob-carpenter",
      "id": 3383807,
      "node_id": "MDQ6VXNlcjMzODM4MDc=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/3383807?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bob-carpenter",
      "html_url": "https://github.com/bob-carpenter",
      "followers_url": "https://api.github.com/users/bob-carpenter/followers",
      "following_url": "https://api.github.com/users/bob-carpenter/following{/other_user}",
      "gists_url": "https://api.github.com/users/bob-carpenter/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bob-carpenter/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bob-carpenter/subscriptions",
      "organizations_url": "https://api.github.com/users/bob-carpenter/orgs",
      "repos_url": "https://api.github.com/users/bob-carpenter/repos",
      "events_url": "https://api.github.com/users/bob-carpenter/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bob-carpenter/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T17:44:31Z",
    "updated_at": "2019-06-19T17:44:31Z",
    "author_association": "CONTRIBUTOR",
    "body": "\n> We still store whole matrix, even if we need only a triangular matrix. This is simpler and prrobably does not affect performance that much. Operations that are faster on triangular matrices were optimized in this way before this PR.\n\nThis is also how Eigen deals with triangular matrices.\n\n\n> What this PR changes is that it lets the matrix track which part of it is actually in use. So that does not need to be specified when calling a function that operates on a triangular part. For example addition of two upper triangular matrices produces an upper triangular matrix. Some functions needed to be changed to only operate on triangular part if it is specified on input matrix. However I expect no or minimal benefits to performance.\n\nWouldn't it be a factor of 2?  Or does it defeat built-in CPU vectorization?\n\nThe bigger deal is in autodiff, where those extra operations go on the autodiff stack and then add unnecessary work to the reverse pass of autodiff."
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503726008",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503726008",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503726008,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzcyNjAwOA==",
    "user": {
      "login": "t4c1",
      "id": 7499452,
      "node_id": "MDQ6VXNlcjc0OTk0NTI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/7499452?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/t4c1",
      "html_url": "https://github.com/t4c1",
      "followers_url": "https://api.github.com/users/t4c1/followers",
      "following_url": "https://api.github.com/users/t4c1/following{/other_user}",
      "gists_url": "https://api.github.com/users/t4c1/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/t4c1/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/t4c1/subscriptions",
      "organizations_url": "https://api.github.com/users/t4c1/orgs",
      "repos_url": "https://api.github.com/users/t4c1/repos",
      "events_url": "https://api.github.com/users/t4c1/events{/privacy}",
      "received_events_url": "https://api.github.com/users/t4c1/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T20:04:14Z",
    "updated_at": "2019-06-19T20:04:14Z",
    "author_association": "CONTRIBUTOR",
    "body": "> Wouldn't it be a factor of 2? Or does it defeat built-in CPU vectorization?\r\n\r\nIm not sure what you mean with defeat built-in CPU vectorization. I think you probably mixed something up, as we are discussing OpenCL code.\r\n\r\n I did not measure speedup. It might be up to 2. These simple kernels are not very optimized anyway, as they are not expected to bottlenecks in many scenarios. In case they are it is usually better to combine addition with as much other operations as possible into a single kernel. This reduces the amount data that needs to be transfered to/from relatively slow global GPU memory."
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503727761",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503727761",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503727761,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzcyNzc2MQ==",
    "user": {
      "login": "bob-carpenter",
      "id": 3383807,
      "node_id": "MDQ6VXNlcjMzODM4MDc=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/3383807?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bob-carpenter",
      "html_url": "https://github.com/bob-carpenter",
      "followers_url": "https://api.github.com/users/bob-carpenter/followers",
      "following_url": "https://api.github.com/users/bob-carpenter/following{/other_user}",
      "gists_url": "https://api.github.com/users/bob-carpenter/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bob-carpenter/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bob-carpenter/subscriptions",
      "organizations_url": "https://api.github.com/users/bob-carpenter/orgs",
      "repos_url": "https://api.github.com/users/bob-carpenter/repos",
      "events_url": "https://api.github.com/users/bob-carpenter/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bob-carpenter/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T20:09:20Z",
    "updated_at": "2019-06-19T20:09:20Z",
    "author_association": "CONTRIBUTOR",
    "body": "I should've said GPU vectorization.  The reason I wonder is that the memory in a triangular matrix isn't contiguous in Eigen's representation. It uses just the upper or lower triangular part of a dense matrix.\n\nI have it on good authority that minimizing back and forth to non-GPU memory is critical.  The Tensorflow Probability devs are trying to put everything on the GPU so you never have to leave the GPU.\n\n"
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503729621",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503729621",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503729621,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzcyOTYyMQ==",
    "user": {
      "login": "t4c1",
      "id": 7499452,
      "node_id": "MDQ6VXNlcjc0OTk0NTI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/7499452?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/t4c1",
      "html_url": "https://github.com/t4c1",
      "followers_url": "https://api.github.com/users/t4c1/followers",
      "following_url": "https://api.github.com/users/t4c1/following{/other_user}",
      "gists_url": "https://api.github.com/users/t4c1/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/t4c1/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/t4c1/subscriptions",
      "organizations_url": "https://api.github.com/users/t4c1/orgs",
      "repos_url": "https://api.github.com/users/t4c1/repos",
      "events_url": "https://api.github.com/users/t4c1/events{/privacy}",
      "received_events_url": "https://api.github.com/users/t4c1/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-19T20:14:53Z",
    "updated_at": "2019-06-19T20:16:55Z",
    "author_association": "CONTRIBUTOR",
    "body": ">  Or does it defeat built-in GPU vectorization?\r\n\r\nNo, my implementation still launches threads for elements that are out of selected triangular parts. These threads just write zeros to appropriate elements of result matrix.\r\n\r\n>I have it on good authority that minimizing back and forth to non-GPU memory is critical.\r\n\r\nThat is true. But I don't understand how it is relevant for this PR."
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/503842603",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-503842603",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 503842603,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwMzg0MjYwMw==",
    "user": {
      "login": "SteveBronder",
      "id": 5857231,
      "node_id": "MDQ6VXNlcjU4NTcyMzE=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/5857231?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/SteveBronder",
      "html_url": "https://github.com/SteveBronder",
      "followers_url": "https://api.github.com/users/SteveBronder/followers",
      "following_url": "https://api.github.com/users/SteveBronder/following{/other_user}",
      "gists_url": "https://api.github.com/users/SteveBronder/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/SteveBronder/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/SteveBronder/subscriptions",
      "organizations_url": "https://api.github.com/users/SteveBronder/orgs",
      "repos_url": "https://api.github.com/users/SteveBronder/repos",
      "events_url": "https://api.github.com/users/SteveBronder/events{/privacy}",
      "received_events_url": "https://api.github.com/users/SteveBronder/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-20T04:42:22Z",
    "updated_at": "2019-06-20T04:42:22Z",
    "author_association": "CONTRIBUTOR",
    "body": "For clarification on this PR, this is a style change (but one that sorely needs done).\r\n\r\nRight now if we want to call the kernels for multiply that accounts for triangularity we have to call\r\n\r\n```cpp\r\nmatrix_cl C = multiply<triangular_view_cl::UPPER,\r\n                       triangular_view_cl::LOWER>(A, B);\r\n```\r\n\r\nBut with this PR we will be able to call `matrix_cl C = A * B`\r\n\r\nUntil the Stan language is very definite on how to generally handle sparse types (something I think I'll be working on with Dan Simpson and Aki starting July) eod we still treat this as a dense matrix with some cleverness taking into account the triangularity\r\n\r\n> I have it on good authority that minimizing back and forth to non-GPU memory is critical.  The Tensorflow Probability devs are trying to put everything on the GPU so you never have to leave the GPU.\r\n\r\nYes def, I've been racking over how to best keep things over there for a while. The best thing imo is that, when we move to Stan 3, Sean and I have talked about doing this within the compiler (i.e. building signatures that take in `matrix_cl` objects when doing rev. That and moving over to OpenCL 2.0 to get non-zero copy buffers for CPUs and APUs (Devices where the CPU and GPU are embedded in one another).\r\n\r\nBut that's a different PR, this one is to make the API much friendlier :)"
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/504485116",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-504485116",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 504485116,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwNDQ4NTExNg==",
    "user": {
      "login": "bob-carpenter",
      "id": 3383807,
      "node_id": "MDQ6VXNlcjMzODM4MDc=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/3383807?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bob-carpenter",
      "html_url": "https://github.com/bob-carpenter",
      "followers_url": "https://api.github.com/users/bob-carpenter/followers",
      "following_url": "https://api.github.com/users/bob-carpenter/following{/other_user}",
      "gists_url": "https://api.github.com/users/bob-carpenter/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bob-carpenter/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bob-carpenter/subscriptions",
      "organizations_url": "https://api.github.com/users/bob-carpenter/orgs",
      "repos_url": "https://api.github.com/users/bob-carpenter/repos",
      "events_url": "https://api.github.com/users/bob-carpenter/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bob-carpenter/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-21T16:20:19Z",
    "updated_at": "2019-06-21T16:20:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "\n> I have it on good authority that minimizing back and forth to non-GPU memory is critical.\n> \n> That is true. But I don't understand how it is relevant for this PR.\n\nYou're right---not relevant for this PR.\n\n"
  },
  {
    "url": "https://api.github.com/repos/stan-dev/math/issues/comments/505044164",
    "html_url": "https://github.com/stan-dev/math/pull/1266#issuecomment-505044164",
    "issue_url": "https://api.github.com/repos/stan-dev/math/issues/1266",
    "id": 505044164,
    "node_id": "MDEyOklzc3VlQ29tbWVudDUwNTA0NDE2NA==",
    "user": {
      "login": "stan-buildbot",
      "id": 3512237,
      "node_id": "MDQ6VXNlcjM1MTIyMzc=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3512237?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/stan-buildbot",
      "html_url": "https://github.com/stan-buildbot",
      "followers_url": "https://api.github.com/users/stan-buildbot/followers",
      "following_url": "https://api.github.com/users/stan-buildbot/following{/other_user}",
      "gists_url": "https://api.github.com/users/stan-buildbot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/stan-buildbot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/stan-buildbot/subscriptions",
      "organizations_url": "https://api.github.com/users/stan-buildbot/orgs",
      "repos_url": "https://api.github.com/users/stan-buildbot/repos",
      "events_url": "https://api.github.com/users/stan-buildbot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/stan-buildbot/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-06-24T14:50:21Z",
    "updated_at": "2019-06-24T14:50:21Z",
    "author_association": "CONTRIBUTOR",
    "body": "(stat_comp_benchmarks/benchmarks/gp_pois_regr/gp_pois_regr.stan, 0.99)\r\n(stat_comp_benchmarks/benchmarks/low_dim_corr_gauss/low_dim_corr_gauss.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/irt_2pl/irt_2pl.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/pkpd/one_comp_mm_elim_abs.stan, 0.99)\r\n(stat_comp_benchmarks/benchmarks/eight_schools/eight_schools.stan, 1.01)\r\n(stat_comp_benchmarks/benchmarks/gp_regr/gp_regr.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/arK/arK.stan, 1.0)\r\n(performance.compilation, 1.01)\r\n(stat_comp_benchmarks/benchmarks/low_dim_gauss_mix_collapse/low_dim_gauss_mix_collapse.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/low_dim_gauss_mix/low_dim_gauss_mix.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/sir/sir.stan, 0.96)\r\n(stat_comp_benchmarks/benchmarks/pkpd/sim_one_comp_mm_elim_abs.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/garch/garch.stan, 1.01)\r\n(stat_comp_benchmarks/benchmarks/gp_regr/gen_gp_data.stan, 1.0)\r\n(stat_comp_benchmarks/benchmarks/arma/arma.stan, 1.0)\r\nResult: 0.99814046556\r\nCommit hash: 16a075453853ee296ba0f0352b659d600df8fdc0\r\n"
  }
]
